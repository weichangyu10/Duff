[{"authors":["W Yu","L Azizi","JT Ormerod"],"categories":null,"content":"","date":1580475600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580475600,"objectID":"caae70970030052c8f733b2ca8421a2b","permalink":"/publication/clothing-search/","publishdate":"2020-02-01T00:00:00+11:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"Variable selection and classification are common objectives in the analysis of high-dimensional data. Most such methods make distributional assumptions that may not be compatible with the diverse families of distributions data can take. A novel Bayesian nonparametric discriminant analysis model that performs both variable selection and classification within a seamless framework is proposed. Polya tree priors are assigned to the unknown group-conditional distributions to account for their uncertainty, and allow prior beliefs about the distributions to be incorporated simply as hyperparameters. The adoption of collapsed variational Bayes inference in combination with a chain of functional approximations led to an algorithm with low computational cost. The resultant decision rules carry heuristic interpretations and are related to an existing two-sample Bayesian nonparametric hypothesis test. By an application to some simulated and publicly available real datasets, the proposed method exhibits good performance when compared to current state-of-the-art approaches. XYduahdsodasfgagbadfadfadjhlvhvliygiyoXYXY","tags":[],"title":"Variational nonparametric discriminant analysis","type":"publication"},{"authors":[],"categories":[],"content":"  Bayesian Inference Suppose we have observed the data \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) that are drawn from the model \\(\\mathcal{P} = \\{ p_i(x_i \\; | \\boldsymbol{\\theta}), \\, 1 \\le i \\le n \\}\\), where \\(\\boldsymbol{\\theta}\\) is a vector of unknown parameters. In Bayesian inference, we express our uncertainty about \\(\\boldsymbol{\\theta}\\) with a prior distribution on the space of all possible values which \\(\\boldsymbol{\\theta}\\) can take and revise our uncertainty using information provided by the observed data. Our objective is to compute the posterior distribution of \\(\\boldsymbol{\\theta}\\) - the conditional distribution of \\(\\boldsymbol{\\theta}\\) given the data \\(\\mathbf{x}\\), \\[\\begin{align*} p (\\boldsymbol{\\theta} | \\mathbf{x}) \u0026amp;= \\frac{p (\\boldsymbol{\\theta}, \\mathbf{x})}{p(\\mathbf{x})}, \\\\ \u0026amp;\\propto K (\\boldsymbol{\\theta}, \\mathbf{x}) \\;\\;\\;\\; (1). \\end{align*}\\] \nThis distribution represents our revised uncertainty about \\(\\boldsymbol{\\theta}\\) after observing the data \\(\\mathbf{x}\\). Notice that we have expressed the posterior distribution up to a proportionality constant in the previous equation. The function \\(K\\) is known as the kernel of the posterior distribution and can be used to identify the distributional family which the posterior belongs to. This allows us to bypass the need to compute the proportionality constant explicitly. However, the kernel function may not belong to any recognisable distributional family. In this case, one has to evaluate the proportionality constant via the integral \\[ \\left \\{ \\int K (\\boldsymbol{\\theta}, \\mathbf{x}) \\, d \\boldsymbol{\\theta} \\right \\}^{-1}. \\] Using the posterior distribution, we may perform inference by computing the posterior moments.\n\nToy example - Gaussian data with unknown variance \nIn this toy example, we will calculate the posterior distribution of the unknown variance of a sample drawn from a normal population centred about \\(0\\), i.e. \\(\\boldsymbol{\\theta} = \\sigma^2\\) and \\[\\begin{align*} p_i(x_i \\; | \\boldsymbol{\\theta}) \u0026amp;= p(x_i \\; | \\boldsymbol{\\theta}), \\\\ \u0026amp;= (2 \\pi \\sigma^2)^{- \\tfrac{1}{2}} \\exp \\left \\{ - \\tfrac{x_i^2}{2 \\sigma^2} \\right \\}. \\end{align*}\\] By assigning an Inverse Gamma prior on \\(\\sigma^2\\) with shape \\(A\\) and scale \\(B\\), we apply equation \\((1)\\) to obtain the posterior distribution for \\(\\sigma^2\\) as \\[\\begin{align*} p(\\sigma^2 | \\mathbf{x}) \u0026amp;\\propto (\\sigma^2) ^{ - (A + n/2) - 1} \\exp \\left \\{ - (B + \\tfrac{1}{2} \\lVert \\mathbf{x} \\rVert^2 )/\\sigma^2 \\right \\},\\\\ \u0026amp;= K(\\sigma^2, \\mathbf{x}) \\;\\;\\;\\; (2). \\end{align*}\\] To obtain the normalising constant for the above posterior, we can compute the integral according to the expression in equation \\((2)\\). However, this turns out to be unnecessary as the kernel \\(K\\) is easily recognisable as that of a Inverse Gamma distribution with shape \\(A + n/2\\) and scale \\(B + \\tfrac{1}{2} \\lVert \\mathbf{x} \\rVert^2\\). Hence, we may write \\[\\begin{equation*} \\sigma^2 \\, | \\, \\mathbf{x} \\sim \\text{InvGa} (A + n/2, B + \\tfrac{1}{2} \\lVert \\mathbf{x} \\rVert^2). \\end{equation*}\\] A natural “point estimate” for \\(\\sigma^2\\) would be the posterior mean, i.e. \\[\\begin{equation*} \\mathbb{E} (\\sigma^2 \\, | \\, \\mathbf{x}) = \\frac{B + \\tfrac{1}{2} \\lVert \\mathbf{x} \\rVert^2}{A + n/2 - 1}. \\end{equation*}\\] To obtain an interval “estimate”, one may use the quantiles of the \\(\\text{InvGa} (A + n/2, B + \\tfrac{1}{2} \\lVert \\mathbf{x} \\rVert^2)\\) distribution.\n ","date":1548288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548288000,"objectID":"78c870e551ecd79f5e2788fa418b2f9d","permalink":"/post/introduction-to-bayesian-inference/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/post/introduction-to-bayesian-inference/","section":"post","summary":"Bayesian Inference Suppose we have observed the data \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\) that are drawn from the model \\(\\mathcal{P} = \\{ p_i(x_i \\; | \\boldsymbol{\\theta}), \\, 1 \\le i \\le n \\}\\), where \\(\\boldsymbol{\\theta}\\) is a vector of unknown parameters. In Bayesian inference, we express our uncertainty about \\(\\boldsymbol{\\theta}\\) with a prior distribution on the space of all possible values which \\(\\boldsymbol{\\theta}\\) can take and revise our uncertainty using information provided by the observed data.","tags":["Bayesian inference"],"title":"Introduction to Bayesian Inference","type":"post"},{"authors":["W Yu","JT Ormerod","M Stewart"],"categories":null,"content":"","date":1535724000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535724000,"objectID":"5cec0ce6e082b377c504bc66cdf990c5","permalink":"/publication/person-re-identification/","publishdate":"2018-09-01T00:00:00+10:00","relpermalink":"/publication/person-re-identification/","section":"publication","summary":"A Bayesian method that seamlessly fuses classification via discriminant analysis and hypothesis testing is developed. Building upon the original discriminant analysis classifier, modelling components are added to identify discriminative variables. A combination of cake priors and a novel form of variational Bayes we call reverse collapsed variational Bayes gives rise to variable selection that can be directly posed as a multiple hypothesis testing approach using likelihood ratio statistics. Some theoretical arguments are presented showing that Chernoff-consistency (asymptotically zero type I and type II error) is maintained across all hypotheses. We apply our method on some publicly available genomic datasets and show that our method performs well in practice. An R package VaDA has also been made available on Github.","tags":[],"title":"Variational discriminant analysis with variable selection","type":"publication"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530108000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530108000,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00+10:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":["JT Ormerod","M Stewart","W Yu","SE Romanes"],"categories":null,"content":"","date":1506780000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506780000,"objectID":"4c31f3279ba237deeadb52c08e3dbf73","permalink":"/publication/cake/","publishdate":"2017-10-01T00:00:00+10:00","relpermalink":"/publication/cake/","section":"publication","summary":"We introduce a new class of priors for  Bayesian hypothesis testing, which we name cake priors. These priors circumvent Bartlett's paradox (also called the Jeffreys-Lindley paradox); the problem associated with the use of diffuse priors leading to nonsensical statistical inferences. Cake priors allow the use of diffuse priors (having one's cake) while achieving theoretically justified inferences (eating it too). We demonstrate this methodology for Bayesian hypotheses tests for scenarios under which the one and two sample t-tests, and linear models are typically derived. The resulting Bayesian test statistic takes the form of a penalized likelihood ratio test statistic. By considering the sampling distribution under the null and alternative hypotheses we show for independent identically distributed regular parametric models that Bayesian hypothesis tests using cake priors are Chernoff-consistent, i.e., achieve zero type I and II errors asymptotically. Lindley's paradox is also discussed. We argue that a true Lindley's paradox  will only occur with small probability for large sample sizes.","tags":[],"title":"Bayesian hypothesis tests with diffuse priors: Can we have our cake and eat it too?","type":"publication"}]